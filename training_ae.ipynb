{"cells":[{"cell_type":"markdown","metadata":{"id":"buqoideQiKJ4"},"source":["# Auto-encoders for Document Denoising"]},{"cell_type":"markdown","metadata":{"id":"wCKQ-4cWiKJ6"},"source":["## About Autoencoders\n","An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name."]},{"cell_type":"markdown","metadata":{"id":"9j5Dk3KLiKJ6"},"source":["![](https://osclasspoint.com/kaggle/autoencoder.png)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install pandas matplotlib tensorflow scikit-learn opencv-python numpy augraphy huggingface_hub\n","\n"," "]},{"cell_type":"markdown","metadata":{},"source":["# Before setup everything, lets try to generate"]},{"cell_type":"markdown","metadata":{},"source":["# if mac"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install tensorflow-metal tensorflow-macos"]},{"cell_type":"markdown","metadata":{"id":"txBpPaH5iKJ7"},"source":["## Import libraries and data\n","First load libraries we need for our work. We need multiple libraries to be able to unzip files, work with directories, sklearn, tensorflow..."]},{"cell_type":"code","execution_count":37,"metadata":{"id":"6vfAM4oQiKJ7"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import zipfile\n","import os\n","import cv2\n","\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Dropout, BatchNormalization, Input\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Dropout\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.regularizers import l2 # import the l2 regularizer from keras.regularizers\n","\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check if GPU is available\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n","\n","# List available physical devices\n","print(\"Available devices:\", tf.config.list_physical_devices())"]},{"cell_type":"markdown","metadata":{"id":"543s5KbViKJ8"},"source":["As we have data zipped, we will have to work in `./data/` directory to unzip images here."]},{"cell_type":"code","execution_count":39,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"HDz7lYu7iKJ8"},"outputs":[],"source":["# path to zipped & working directories\n","path_zip = 'zip/'\n","path = 'data/'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from huggingface_hub import hf_hub_download\n","#https://huggingface.co/datasets/manubaun/receipt/resolve/main/Archiv.zip\n","# https://huggingface.co/docs/huggingface_hub/guides/download\n","trainingfile  = hf_hub_download(repo_id=\"manubaun/receipt\", filename=\"training_set_20000.zip\", repo_type=\"dataset\")\n","testfile  = hf_hub_download(repo_id=\"manubaun/receipt\", filename=\"test.zip\", repo_type=\"dataset\")\n"]},{"cell_type":"code","execution_count":82,"metadata":{"id":"dSLpvfcRiKJ8"},"outputs":[],"source":["# unzip files first to working directory\n","with zipfile.ZipFile(testfile, 'r') as zip_ref:\n","    zip_ref.extractall(path)  \n","  \n","with zipfile.ZipFile(trainingfile, 'r') as zip_ref:\n","    zip_ref.extractall(path)"]},{"cell_type":"markdown","metadata":{"id":"EA7R9kIyiKJ8"},"source":["For later use, we will store image names into list, so we can draw them simply."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"mcsEFsU_iKJ9"},"outputs":[],"source":["# store image names in list for later use\n","train_img = sorted(os.listdir( path+'train/'))\n","train_cleaned_img = sorted(os.listdir( path+'train_cleaned/'))\n","test_img = sorted(os.listdir(path+'test/'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(train_img),len(train_cleaned_img),len(test_img)"]},{"cell_type":"markdown","metadata":{"id":"wHeUOFDHiKJ9"},"source":["## Data preparation\n","Next step is to define function to process images and then store this images in list. As there is not as many data, we do not need to work in batches."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"X9nRriQqiKJ9"},"outputs":[],"source":["IMG_WIDTH = 400\n","IMG_HEIGHT = 400\n","\n","# prepare function\n","def process_image(path):\n","    img = cv2.imread(path)\n","    img = np.asarray(img, dtype=\"float32\")\n","    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    img = img/255.0\n","    img = np.reshape(img, (IMG_HEIGHT, IMG_WIDTH, 1))\n","    \n","    return img"]},{"cell_type":"markdown","metadata":{"id":"uuWn45SyiKJ9"},"source":["Reshape images and put them into list."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"l4w3qrPziKJ9"},"outputs":[],"source":["# preprocess images\n","train = []\n","train_cleaned = []\n","test = []\n","\n","for f in train_img:\n","    train.append(process_image(path+\"train/\" + f))\n","\n","for f in train_cleaned_img:\n","    train_cleaned.append(process_image(path+'train_cleaned/' + f))\n","    \n","for f in test_img:\n","    test.append(process_image(path + 'test/' + f))\n"]},{"cell_type":"markdown","metadata":{"id":"Pvwxba5piKJ-"},"source":["## Exploratory data analysis\n","Not too much to look there, but just quickly look on train images and their cleaned version. This is what we put into model to learn how to clean noise from background."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1815,"status":"ok","timestamp":1655568425729,"user":{"displayName":"Michael S","userId":"03226043462629562395"},"user_tz":-60},"id":"PbV6PasriKJ-","outputId":"0741cb4d-d746-4da7-e300-7faf9f2bcd6d"},"outputs":[],"source":["plt.figure(figsize=(15,25))\n","for i in range(0,8,2):\n","    plt.subplot(4,2,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.imshow(train[i][:,:,0], cmap='gray')\n","    plt.title('Noise image: {}'.format(train_img[i]))\n","    \n","    plt.subplot(4,2,i+2)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.imshow(train_cleaned[i][:,:,0], cmap='gray')\n","    plt.title('Denoised image: {}'.format(train_img[i]))\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ayGOEWWaiKJ_"},"source":["## Split data\n","In this step we convert lists to numpy arrays and split dataset into train and validation in ration 85% train, 15% test."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"E3qQ82FuiKJ_"},"outputs":[],"source":["# convert list to numpy array\n","X_train = np.asarray(train)\n","Y_train = np.asarray(train_cleaned)\n","X_test = np.asarray(test)\n","\n","X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.15)"]},{"cell_type":"markdown","metadata":{"id":"qFiab7uAiKJ_"},"source":["## Modeling\n","It's nice to look at model summary how it works in terms of layer sizes: 420x540 -> 210x270 --> 210x270 -> 420x540"]},{"cell_type":"markdown","metadata":{"id":"9AOfev19iKJ_"},"source":["Adam as optimizer is used (as it worked best out of other optimizers), loss is based on mean squared error and we are looking on mean absolute error as well."]},{"cell_type":"markdown","metadata":{},"source":["## Updated Model Version via Chatgpt"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Dropout, Activation, add, Concatenate\n","from tensorflow.keras.models import Model\n","\n","def model():\n","    \"\"\"Improved model with increased depth, skip connections, and batch normalization.\"\"\"\n","    input_layer = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 1))\n","    \n","    # Encoder\n","    # Block 1\n","    x = Conv2D(64, (3, 3), padding='same')(input_layer)\n","    x = Activation('relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Conv2D(64, (3, 3), padding='same')(x)\n","    x = Activation('relu')(x)\n","    x = BatchNormalization()(x)\n","    skip1 = x  # Skip connection\n","    x = MaxPooling2D((2, 2), padding='same')(x)\n","    x = Dropout(0.5)(x)\n","    \n","    # Block 2\n","    x = Conv2D(128, (3, 3), padding='same')(x)\n","    x = Activation('relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Conv2D(128, (3, 3), padding='same')(x)\n","    x = Activation('relu')(x)\n","    x = BatchNormalization()(x)\n","    skip2 = x  # Skip connection\n","    x = MaxPooling2D((2, 2), padding='same')(x)\n","    x = Dropout(0.5)(x)\n","    \n","    # Bottleneck\n","    x = Conv2D(256, (3, 3), padding='same')(x)\n","    x = Activation('relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Conv2D(256, (3, 3), padding='same')(x)\n","    x = Activation('relu')(x)\n","    x = BatchNormalization()(x)\n","    \n","    # Decoder\n","    # Block 2\n","    x = UpSampling2D((2, 2))(x)\n","    x = Concatenate()([x, skip2])  # Skip connection\n","    x = Conv2D(128, (3, 3), padding='same')(x)\n","    x = Activation('relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Conv2D(128, (3, 3), padding='same')(x)\n","    x = Activation('relu')(x)\n","    x = BatchNormalization()(x)\n","    \n","    # Block 1\n","    x = UpSampling2D((2, 2))(x)\n","    x = Concatenate()([x, skip1])  # Skip connection\n","    x = Conv2D(64, (3, 3), padding='same')(x)\n","    x = Activation('relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Conv2D(64, (3, 3), padding='same')(x)\n","    x = Activation('relu')(x)\n","    x = BatchNormalization()(x)\n","    \n","    # Output Layer\n","    output_layer = Conv2D(1, (1, 1), activation='sigmoid', padding='same')(x)\n","    \n","    # Model Compilation\n","    optimizer = Adam(learning_rate=0.001) \n","    model = Model(inputs=input_layer, outputs=output_layer)\n","    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","    \n","    return model\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1NlQq3bbiKKA"},"source":["### Train model\n","We will run 80 epochs having early stopping set to 10 (if val loss does not drop in 10 epochs, it will stop)."]},{"cell_type":"markdown","metadata":{"id":"XLipSucdiKKA"},"source":["Let's store history of model as well, so we can plot loss (rmse) and mae."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = model()\n","model.summary()"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","\n","class SaveModelCallback(tf.keras.callbacks.Callback):\n","    def __init__(self, save_dir):\n","        super(SaveModelCallback, self).__init__()\n","        self.save_dir = save_dir\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        # Save the model at the end of each epoch\n","        model_path = f\"{self.save_dir}/model_epoch_{epoch + 1}.keras\"\n","        self.model.save(model_path)\n","        print(f\"Model saved to {model_path}\")\n","        \n","# Define your save directory\n","save_directory = 'checkpoints'\n","\n","# Create the callback\n","save_model_callback = SaveModelCallback(save_dir=save_directory)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":682388,"status":"ok","timestamp":1655569269708,"user":{"displayName":"Michael S","userId":"03226043462629562395"},"user_tz":-60},"id":"meGQ3kmDiKKA","outputId":"c629df5d-162b-471f-f01d-5a0dd33ccbcc","scrolled":true},"outputs":[],"source":["early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10,min_lr=1e-7, verbose=1)\n","\n","#callback = EarlyStopping(monitor='loss', patience=10)\n","history = model.fit(X_train, Y_train, \n","                    validation_data = (X_val, Y_val), \n","                    epochs=100, \n","                    batch_size=16, \n","                    shuffle=True,\n","                    callbacks=[early_stopping,lr_scheduler, save_model_callback])\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Save model\n","\n","save the model"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["model.save_weights('./model_end.keras')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":668,"status":"ok","timestamp":1655633142088,"user":{"displayName":"Michael S","userId":"03226043462629562395"},"user_tz":-60},"id":"TZMue83qiKKB","outputId":"4fd91f50-33fa-44c4-90ce-1e825ecc0295"},"outputs":[],"source":["# Restore the weights\n","model.load_weights('./checkpoints/autoencoders/model_epoch_1.weights.h5')"]},{"cell_type":"markdown","metadata":{"id":"mxTHWjbqiKKB"},"source":["### Plot error evolution on epochs\n","You may notice jump in error after approx. 5 epoch that is pretty important, but enought epochs flatten this to almost 0."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":376},"executionInfo":{"elapsed":921,"status":"ok","timestamp":1655569280784,"user":{"displayName":"Michael S","userId":"03226043462629562395"},"user_tz":-60},"id":"XEaVUwCNiKKB","outputId":"24d94555-675c-4dc4-a6f8-842324b61ba5"},"outputs":[],"source":["# Check how loss & mae went down\n","epoch_loss = history.history['loss']\n","epoch_val_loss = history.history['val_loss']\n","epoch_mae = history.history['mae']\n","epoch_val_mae = history.history['val_mae']\n","\n","plt.figure(figsize=(20,6))\n","plt.subplot(1,2,1)\n","plt.plot(range(0,len(epoch_loss)), epoch_loss, 'b-', linewidth=2, label='Train Loss')\n","plt.plot(range(0,len(epoch_val_loss)), epoch_val_loss, 'r-', linewidth=2, label='Val Loss')\n","plt.title('Evolution of loss on train & validation datasets over epochs')\n","plt.legend(loc='best')\n","\n","plt.subplot(1,2,2)\n","# plt.plot(range(0,len(epoch_mae)), epoch_mae, 'b-', linewidth=2, label='Train MAE')\n","plt.plot(range(0,len(epoch_val_mae)), epoch_val_mae, 'r-', linewidth=2,label='Val MAE')\n","plt.title('Evolution of MAE on train & validation datasets over epochs')\n","plt.legend(loc='best')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"iVR2dOLhiKKC"},"source":["## Evaluation\n","In this step we will \"predict\", or better say clean test images and check how well model works."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EiKUMJ07iKKD"},"outputs":[],"source":["# predict/clean test images\n","Y_test = model.predict(X_test[:4])"]},{"cell_type":"markdown","metadata":{"id":"FOqfbgBGiKKD"},"source":["Now compare noisy (left) and denoised test images (right). Our model has done great job with denoising!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":4816,"status":"ok","timestamp":1655633247358,"user":{"displayName":"Michael S","userId":"03226043462629562395"},"user_tz":-60},"id":"gJJRtD1siKKD","outputId":"f07d2e33-18ef-499e-8579-1b93bbc2852e"},"outputs":[],"source":["plt.figure(figsize=(15,25))\n","\n","for i in range(0,8,2):\n","  index = int(i/2)\n","  plt.subplot(4,2,i+1)\n","  plt.xticks([])\n","  plt.yticks([])\n","  plt.imshow(X_test[index][:,:,0], cmap='gray')\n","  plt.title('Noisy image: {}'.format(test_img[index]))\n","  \n","  plt.subplot(4,2,i+2)\n","  plt.xticks([])\n","  plt.yticks([])\n","  plt.imshow(Y_test[index][:,:,0], cmap='gray')\n","  plt.title('Denoised by autoencoder: {}'.format(test_img[index]))\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"l23tF37eFvFm"},"source":["Check the performance on other kind of document - invoice, that has multiple kinds of noise in daily lives. It's shown that it could perform well on half of the images."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def visualize_model_predictions(model_path, X_val):\n","    model = tf.keras.models.load_model(model_path)\n","    predictions = model.predict(X_val)\n","\n","    # Assuming you want to visualize the first 5 predictions\n","    plt.figure(figsize=(15, 5))\n","    for i in range(5):\n","        plt.subplot(1, 5, i + 1)\n","        plt.imshow(X_test[i].reshape(400, 400), cmap='gray')  # Adjust reshape based on your input shape\n","        plt.title(f\"Pred: {np.argmax(predictions[i])}\")  # Adjust according to your output\n","        plt.axis('off')\n","    plt.show()\n","\n","\n","# Y_test = model.predict(X_test[:4])\n","# Load and visualize the models\n","for epoch in range(6):  # Assuming you trained for 100 epochs\n","    model_path = f\"{save_directory}/model_epoch_{epoch + 1}.weights.h5\"\n","    visualize_model_predictions(model_path, X_test[:1])"]},{"cell_type":"markdown","metadata":{"id":"jli9wIuumGFw"},"source":["## Next steps\n","- Training the model on a larger dataset\n","- Tuning parameters to achieve greater performance\n","- Fine-tuning the models on a different dataset to implement more functions (e.g., watermark removal and motion deblur)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","from PIL import Image\n","import numpy as np\n","\n","def split_image(image):\n","    patches = []\n","    img_width, img_height = image.size  # (width, height)\n","    patch_width, patch_height = 400, 400  # (width, height)\n","\n","    row_index = 0\n","    for top in range(0, img_height, patch_height):\n","        bottom = min(top + patch_height, img_height)\n","        col_index = 0\n","        for left in range(0, img_width, patch_width):\n","            right = min(left + patch_width, img_width)\n","            box = (left, top, right, bottom)\n","            patch = image.crop(box)\n","            patch_array = np.asarray(patch, dtype=np.float32)\n","\n","            # Pad patch if it's smaller than patch size\n","            if patch_array.shape[0] < patch_height or patch_array.shape[1] < patch_width:\n","                padded_patch = np.zeros((patch_height, patch_width), dtype=np.float32)\n","                padded_patch[:patch_array.shape[0], :patch_array.shape[1]] = patch_array\n","                patch_array = padded_patch\n","\n","            patches.append((patch_array, (row_index, col_index)))\n","            col_index += 1\n","        row_index += 1\n","\n","    return patches\n","\n","def reassemble_image(patches, original_size):\n","    patch_height, patch_width = patches[0][0].shape  # (height, width)\n","\n","    max_row = max(position[0] for _, position in patches)\n","    max_col = max(position[1] for _, position in patches)\n","\n","    reconstructed_height = (max_row + 1) * patch_height\n","    reconstructed_width = (max_col + 1) * patch_width\n","\n","    reconstructed_image = np.zeros((reconstructed_height, reconstructed_width), dtype=np.float32)\n","\n","    for patch, (row, col) in patches:\n","        reconstructed_image[row * patch_height: (row + 1) * patch_height,\n","                            col * patch_width: (col + 1) * patch_width] = patch\n","\n","    # Crop the reconstructed image to the original size\n","    orig_width, orig_height = original_size  # (width, height)\n","    reconstructed_image = reconstructed_image[:orig_height, :orig_width]\n","\n","    return reconstructed_image.astype(np.uint8)\n","\n","# Create output directory if it doesn't exist\n","output_dir = path+'/images_denoised'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Path to the test_invoice folder\n","input_dir = path+'/images_test'\n","\n","# Iterate over all image files in the folder\n","for filename in os.listdir(input_dir):\n","    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n","        input_path = os.path.join(input_dir, filename)\n","        print(f\"Processing {input_path}...\")\n","\n","        # Load the image\n","        image = Image.open(input_path).convert('L')  # Convert to grayscale if needed\n","\n","        # Save original image size\n","        original_size = image.size  # (width, height)\n","\n","        # Split the image into patches\n","        patches = split_image(image)\n","\n","        # Process each patch (e.g., denoising)\n","        denoised_patches = []\n","        for patch_array, position in patches:\n","            # Prepare the patch for the model\n","            patch_input = patch_array.reshape(1, patch_array.shape[0], patch_array.shape[1], 1) / 255.0\n","\n","            # Process the patch with your model\n","            denoised_patch = model.predict(patch_input)\n","            denoised_patch = denoised_patch.squeeze() * 255.0  # Denormalize\n","\n","            # Ensure the denoised patch is the correct size\n","            denoised_patch = denoised_patch.astype(np.float32)\n","            denoised_patches.append((denoised_patch, position))\n","\n","        # Reassemble the processed patches into an image\n","        reconstructed_image_array = reassemble_image(denoised_patches, original_size)\n","\n","        # Convert the NumPy array back to a PIL image\n","        reconstructed_image = Image.fromarray(reconstructed_image_array)\n","\n","        # Save the reconstructed image\n","        output_path = os.path.join(output_dir, f\"denoised_{filename}\")\n","        reconstructed_image.save(output_path)\n"]}],"metadata":{"accelerator":"TPU","colab":{"name":"autoencoders.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
